#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§äº¤é€šæµé‡é¢„æµ‹æ¨¡å‹æœåŠ¡å™¨
æä¾›åŸºäºXGBoostçš„èŠ‚ç‚¹æµé‡é¢„æµ‹API
"""

from flask import Flask, request, jsonify, make_response
import xgboost as xgb
import pandas as pd
import os
import logging
import traceback
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('model_server.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# å…¨å±€æ¨¡å‹å˜é‡
model = None
MODEL_PATH = "models/node_volume_model.json"

def load_model():
    """åŠ è½½XGBoostæ¨¡å‹"""
    global model
    try:
        if os.path.exists(MODEL_PATH):
            model = xgb.XGBRegressor()
            model.load_model(MODEL_PATH)
            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {MODEL_PATH}")
            return True
        else:
            logger.error(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {MODEL_PATH}")
            return False
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return False

def validate_parameters(node, time):
    """éªŒè¯è¾“å…¥å‚æ•°"""
    try:
        node = int(node)
        time = int(time)
        
        if node <= 0:
            return None, None, "èŠ‚ç‚¹IDå¿…é¡»å¤§äº0"
        
        if not (0 <= time <= 23):
            return None, None, "æ—¶é—´ç‚¹å¿…é¡»åœ¨0-23ä¹‹é—´"
            
        return node, time, None
    except (ValueError, TypeError):
        return None, None, "èŠ‚ç‚¹IDå’Œæ—¶é—´ç‚¹å¿…é¡»æ˜¯æ•´æ•°"

@app.route("/", methods=["GET"])
def home():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return jsonify({
        "service": "Traffic Flow Prediction Model Server",
        "status": "running",
        "model_loaded": model is not None,
        "timestamp": datetime.now().isoformat(),
        "version": "2.0.0"
    })

@app.route("/predict", methods=["GET", "POST"])
def predict_volume():
    """é¢„æµ‹èŠ‚ç‚¹æµé‡"""
    start_time = datetime.now()
    
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        if model is None:
            logger.error("æ¨¡å‹æœªåŠ è½½ï¼Œå°è¯•é‡æ–°åŠ è½½")
            if not load_model():
                return jsonify({
                    "error": "æ¨¡å‹æœåŠ¡ä¸å¯ç”¨",
                    "code": "MODEL_NOT_LOADED"
                }), 503
        
        # è·å–å‚æ•°
        if request.method == "POST":
            data = request.get_json()
            if not data:
                return jsonify({"error": "è¯·æ±‚ä½“ä¸ºç©º"}), 400
            node = data.get("node")
            time = data.get("time")
        else:  # GET è¯·æ±‚
            node = request.args.get("node")
            time = request.args.get("time")
        
        # éªŒè¯å‚æ•°
        node, time, error = validate_parameters(node, time)
        if error:
            return jsonify({"error": error}), 400
        
        # å‡†å¤‡æ•°æ®å¹¶é¢„æµ‹
        df = pd.DataFrame([[node, time]], columns=["node", "time"])
        prediction = model.predict(df)[0]
        
        # ç¡®ä¿é¢„æµ‹å€¼åˆç†
        volume = max(float(prediction), 1.0)
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time = (datetime.now() - start_time).total_seconds() * 1000
        
        logger.info(f"é¢„æµ‹æˆåŠŸ - èŠ‚ç‚¹: {node}, æ—¶é—´: {time}, æµé‡: {volume:.2f}, è€—æ—¶: {processing_time:.2f}ms")
        
        return jsonify({
            "node": node,
            "time": time,
            "volume": volume,
            "processing_time_ms": round(processing_time, 2),
            "timestamp": datetime.now().isoformat(),
            "success": True
        })
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"é¢„æµ‹å¤±è´¥: {error_msg}")
        logger.error(traceback.format_exc())
        
        return jsonify({
            "error": f"é¢„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {error_msg}",
            "code": "PREDICTION_ERROR",
            "success": False
        }), 500

@app.route("/predict/batch", methods=["POST"])
def predict_batch():
    """æ‰¹é‡é¢„æµ‹å¤šä¸ªèŠ‚ç‚¹æµé‡"""
    start_time = datetime.now()
    
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        if model is None:
            if not load_model():
                return jsonify({
                    "error": "æ¨¡å‹æœåŠ¡ä¸å¯ç”¨",
                    "code": "MODEL_NOT_LOADED"
                }), 503
        
        data = request.get_json()
        if not data:
            return jsonify({"error": "è¯·æ±‚ä½“ä¸ºç©º"}), 400
        
        node_ids = data.get("nodeIds", [])
        time_point = data.get("timePoint")
        
        if not node_ids:
            return jsonify({"error": "èŠ‚ç‚¹IDåˆ—è¡¨ä¸èƒ½ä¸ºç©º"}), 400
        
        # éªŒè¯æ—¶é—´ç‚¹
        _, time_point, error = validate_parameters(1, time_point)
        if error:
            return jsonify({"error": f"æ—¶é—´ç‚¹é”™è¯¯: {error}"}), 400
        
        # æ‰¹é‡é¢„æµ‹
        predictions = {}
        for node_id in node_ids:
            try:
                node_id = int(node_id)
                if node_id <= 0:
                    continue
                    
                df = pd.DataFrame([[node_id, time_point]], columns=["node", "time"])
                prediction = model.predict(df)[0]
                predictions[str(node_id)] = max(float(prediction), 1.0)
            except Exception as e:
                logger.warning(f"èŠ‚ç‚¹ {node_id} é¢„æµ‹å¤±è´¥: {str(e)}")
                continue
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time = (datetime.now() - start_time).total_seconds() * 1000
        
        logger.info(f"æ‰¹é‡é¢„æµ‹æˆåŠŸ - èŠ‚ç‚¹æ•°: {len(predictions)}, æ—¶é—´: {time_point}, è€—æ—¶: {processing_time:.2f}ms")
        
        return jsonify({
            "timePoint": time_point,
            "predictions": predictions,
            "count": len(predictions),
            "processing_time_ms": round(processing_time, 2),
            "timestamp": datetime.now().isoformat(),
            "success": True
        })
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"æ‰¹é‡é¢„æµ‹å¤±è´¥: {error_msg}")
        logger.error(traceback.format_exc())
        
        return jsonify({
            "error": f"æ‰¹é‡é¢„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {error_msg}",
            "code": "BATCH_PREDICTION_ERROR",
            "success": False
        }), 500

@app.route("/status", methods=["GET"])
def get_status():
    """è·å–æœåŠ¡çŠ¶æ€"""
    return jsonify({
        "status": "healthy",
        "model_loaded": model is not None,
        "model_path": MODEL_PATH,
        "model_exists": os.path.exists(MODEL_PATH),
        "timestamp": datetime.now().isoformat(),
        "uptime": "è¿è¡Œä¸­"
    })

@app.route("/reload", methods=["POST"])
def reload_model():
    """é‡æ–°åŠ è½½æ¨¡å‹"""
    logger.info("æ”¶åˆ°æ¨¡å‹é‡è½½è¯·æ±‚")
    success = load_model()
    
    return jsonify({
        "success": success,
        "message": "æ¨¡å‹é‡è½½æˆåŠŸ" if success else "æ¨¡å‹é‡è½½å¤±è´¥",
        "timestamp": datetime.now().isoformat()
    }), 200 if success else 500

@app.errorhandler(404)
def not_found(error):
    return jsonify({
        "error": "APIç«¯ç‚¹ä¸å­˜åœ¨",
        "code": "NOT_FOUND",
        "available_endpoints": ["/", "/predict", "/predict/batch", "/status", "/reload"]
    }), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({
        "error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯",
        "code": "INTERNAL_ERROR"
    }), 500

# å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
@app.before_first_request
def initialize():
    logger.info("ğŸš€ å¯åŠ¨äº¤é€šæµé‡é¢„æµ‹æ¨¡å‹æœåŠ¡å™¨...")
    load_model()

if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨äº¤é€šæµé‡é¢„æµ‹æ¨¡å‹æœåŠ¡å™¨...")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    
    # åˆå§‹åŠ è½½æ¨¡å‹
    load_model()
    
    # å¯åŠ¨Flaskåº”ç”¨
    app.run(
        host='0.0.0.0',
        port=5000,
        debug=False,  # ç”Ÿäº§ç¯å¢ƒè®¾ä¸ºFalse
        threaded=True  # æ”¯æŒå¤šçº¿ç¨‹
    ) 